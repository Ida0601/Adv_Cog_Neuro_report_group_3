---
title: "BehaviouralAnalysis"
output: html_document
---

```{r}
# Setup
library(tidyr)
library(dplyr)
remotes::install_version("ggplot2", version = "3.4.2", repos = "http://cran.us.r-project.org")
library(ggplot2)
library(tidyverse)
# install.packages('lme4')
library(lme4)
# Install a previous version
remotes::install_version("emmeans", version = "1.8.2", repos = "http://cran.us.r-project.org")
library(emmeans)
packageVersion("emmeans") 

```

# Load data
```{r}
getwd()
# name path - CHECK FOR YOU
path <- "../../MEG_data/workshop_data/behavioural_logs"

df <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0163_2025_09_22_144412_experiment_data.csv",row.names = NULL)
df1 <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0165_2025_09_23_140007_experiment_data.csv",row.names = NULL)
df2 <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0166_2025_09_23_170126_experiment_data.csv",row.names = NULL)
df3 <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0167_2025_09_22_115315_experiment_data.csv",row.names = NULL)
df4 <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0168_2025_09_24_135504_experiment_data.csv",row.names = NULL)
df5 <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0169_2025_09_23_110138_experiment_data.csv",row.names = NULL)
df6 <- read.csv("../../MEG_data/workshop_data/behavioural_logs/0170_2025_09_24_121645_experiment_data.csv",row.names = NULL)

```
Merge CSVs
```{r}
#merge all data frames
total <- rbind(df, df1, df2, df3, df4, df5, df6)


# correct aligning of column names (remove column called row.names)
names(total)[1:(ncol(total)-1)] <- names(total)[2:ncol(total)]
total[, ncol(total)] <- NULL

```

```{r}
# Create correctness column (1 = correct, 0 = incorrect)
total$correct <- ifelse(total$target_type == total$objective_response, 1, 0)

# Change column type
total$subjective_response <- factor(total$subjective_response)
total$correct <- as.numeric(total$correct)


# Check total no. instances of each PAS
table(total$subjective_response)
```

Sanity Check
```{r}
# Check whether RT patterns align with perceptual clarity.
total$logRT <- scale(log(total$objective_response_time_ms))

SCmodel <- lmer(logRT ~ subjective_response + (1|subject), data= total)

summary(SCmodel)
```
The estimates decrease the higher the PAS in the fixed effects so logRT does get smaller as perceptual awareness increases.In linear mixed models, t-values > ~2 in absolute value generally correspond to p < 0.05. Here, the t-values are much larger in absolute value. So compared to the baseline of PAS1, all the PAS levels are associated with significantly shorter logRTs.

```{r}
# plot
# Compute mean RT per subject and PAS
subject_df <- total %>%
  group_by(subject, subjective_response) %>%
  summarise(mean_RT = mean(objective_response_time_ms))

ggplot(subject_df, aes(x = subjective_response, y = mean_RT, group = subject)) +
  geom_line(alpha = 0.5) +  # lines for each subject
  geom_point(alpha = 0.5) +
  stat_summary(aes(group=1), fun=mean, geom="line", color="red", size=1.5) + # group mean
  stat_summary(aes(group=1), fun=mean, geom="point", color="red", size=3) +
  labs(x = "PAS level", y = "Reaction time (ms)",
       title = "Reaction time vs PAS (individual subjects and group mean)") +
  theme_minimal()

```


Linear Mixed Effects Model
```{r}
# Test assumptions

```

```{r}
# Build model - Subjective_response = PAS score
model <- glmer(correct ~ subjective_response + (subjective_response|subject), 
               data = total, family = binomial)

model2 <- glmer(correct ~ subjective_response + (1|subject), 
               data = total, family = binomial)

model3 <- glmer(correct ~ subjective_response + (0+ subjective_response|subject), 
               data = total, family = binomial)

# Log transform RT - include these in the model.
## Including RT partials out variance due to decision speed, so your PAS coefficient reflects sensitivity independent of RT
total$logRT <- scale(log(total$objective_response_time_ms))  # scaling helps convergence

model4 <- glmer(correct ~ subjective_response + logRT + (subjective_response|subject), 
               data = total, family = binomial)

model5 <- glmer(correct ~ subjective_response * logRT + (subjective_response|subject), 
               data = total, family = binomial)


summary(model)
summary(model2)
summary(model3)
summary(model4)
summary(model5)

BIC(model, model2, model3, model4, model5)
```

```{r}
# Estimated probabilities per PAS level
emm <- emmeans(model5, ~ subjective_response, type = "response")
emm
```
type = "response" gives probabilities (0–1), not log-odds. You’ll see predicted probability of being correct for each PAS level.

This will output:
Difference in log-odds (or odds ratios or is it probabilites?) between every PAS level.
p-values for significance of each pairwise difference.

The plot is in probabilities - probability at each PAS level of a correct answer.
```{r}
# Pairwise differences
pairs(emm, adjust= 'Tukey')

plot(emm)
```


```{r}
emm_df <- as.data.frame(emm)

ggplot(emm_df, aes(x = subjective_response, y = prob)) +
  geom_point(size=3) +
  geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), width=0.2) +
  labs(x = "PAS level", y = "Estimated probability correct",
       title = "Estimated marginal means with 95% CI") +
  theme_minimal()
```
Where the blue lines overlap harder to infer that there is a clear difference. Blue lines are error margin? So probability of a correct response in PAS 3 and 4 responses are VERY similar. There is a jump down to PAS 2 but probability is still high and error margin is quite large and potentially overlaps with error from PAS3. PAS1 still has above chance of having a correct answer! But has no overlap with the other PAS levels, so significantly different? The odds of being correct increase sharply from PAS 1 → 3, then plateau at PAS 4.
Is this on log odds scale? 


## Test PAS1 baseline - Exhaustiveness of the PASscoring. Is it significantly above baseline/chance for PAS1 - if yes then there is residual types of perceptual degree the system does not label. If at chance then it is exhaustive - good with those 4 labels...

```{r}
# Extract intercept log-odds 
intercept_logodds <- fixef(model5)["(Intercept)"]

# Convert log-odds to probability
intercept_prob <- 1 / (1 + exp(-intercept_logodds)) #The model summary is in log odds so need to convert it...
cat("Baseline probability (PAS = 1):", round(intercept_prob, 3), "\n")

# Confidence interval using emmeans
# Estimated marginal means (probabilities)
emm <- emmeans(model5, ~ subjective_response, type = "response")

# Summarize
emm_sum <- summary(emm)

# Extract PAS = 1
pas1 <- emm_sum[emm_sum$subjective_response == "1", ]

# Probability and 95% CI
prob <- pas1$prob
lower <- pas1$asymp.LCL
upper <- pas1$asymp.UCL

cat("Probability of correct at PAS = 1:", round(prob, 3), "\n")
cat("95% CI:", round(lower, 3), "-", round(upper, 3), "\n")




# 4. Check significance vs 0.5
# log-odds for 0.5 probability = 0
# Wald z-test for intercept
se_intercept <- sqrt(vcov(model5)["(Intercept)", "(Intercept)"])
z_value <- (intercept_logodds - 0)/se_intercept
p_value <- 2 * (1 - pnorm(abs(z_value)))
cat("Wald test: z =", round(z_value, 3), ", p =", round(p_value, 5), "\n")
```
The estimated probability of a correct response when the subjective awareness rating (PAS) is 1 is 0.557, i.e., about 56%.
This is slightly above chance (0.5), suggesting that participants are just above random guessing at the lowest level of perceptual awareness. However, given that the confidence interval spans over 0.5 (chance leve) we can't conclude with certainty that performance at PAS 1 is above chance. The Wald test shows that the intercept (baseline log-odds for PAS =1) does not significantly differ from a log-odds of 0 (probability =0.5)
p = 0.07132 > 0.05 → not statistically significant at the conventional 5% level.

Interpretation: There is no resounding evidence that performance at PAS = 1 differs from chance.

This suggests that the PAS scoring system is exhaustive → PAS effectively captures all conscious perceptual content → exhaustive.
